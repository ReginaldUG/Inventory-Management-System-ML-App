{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91be1377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 28597 entries, 0 to 28596\n",
      "Data columns (total 13 columns):\n",
      " #   Column                      Non-Null Count  Dtype         \n",
      "---  ------                      --------------  -----         \n",
      " 0   date                        28597 non-null  datetime64[ns]\n",
      " 1   article                     28597 non-null  object        \n",
      " 2   Quantity                    28597 non-null  float64       \n",
      " 3   unit_price                  28597 non-null  float64       \n",
      " 4   day_of_the_week             28597 non-null  int32         \n",
      " 5   month                       28597 non-null  int32         \n",
      " 6   season                      28597 non-null  object        \n",
      " 7   season_numeric              28597 non-null  int64         \n",
      " 8   article_encoded             28597 non-null  int64         \n",
      " 9   price_article_interaction   28597 non-null  float64       \n",
      " 10  season_article_interaction  28597 non-null  int64         \n",
      " 11  weekly_avg_sales            28597 non-null  float64       \n",
      " 12  monthly_avg_sales           28597 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(5), int32(2), int64(3), object(2)\n",
      "memory usage: 2.6+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 28597 entries, 0 to 28596\n",
      "Data columns (total 13 columns):\n",
      " #   Column                      Non-Null Count  Dtype         \n",
      "---  ------                      --------------  -----         \n",
      " 0   date                        28597 non-null  datetime64[ns]\n",
      " 1   article                     28597 non-null  object        \n",
      " 2   Quantity                    28597 non-null  float64       \n",
      " 3   unit_price                  28597 non-null  float64       \n",
      " 4   day_of_the_week             28597 non-null  int32         \n",
      " 5   month                       28597 non-null  int32         \n",
      " 6   season                      28597 non-null  object        \n",
      " 7   season_numeric              28597 non-null  int64         \n",
      " 8   article_encoded             28597 non-null  int64         \n",
      " 9   price_article_interaction   28597 non-null  float64       \n",
      " 10  season_article_interaction  28597 non-null  int64         \n",
      " 11  weekly_avg_sales            28597 non-null  float64       \n",
      " 12  monthly_avg_sales           28597 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(5), int32(2), int64(3), object(2)\n",
      "memory usage: 2.6+ MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import holidays\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, KFold, RandomizedSearchCV, ParameterGrid, TimeSeriesSplit, train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "#   File paths\n",
    "file_path_EDA = \"C:/Users/regi4/OneDrive - Atlantic TU/Year 4/Predictive Analysis Docu's/THE MODEL/French bakery daily sales/Iterations Output/french_bakery_EDA_2.csv\"\n",
    "file_path_Feat_Engr = \"C:/Users/regi4/OneDrive - Atlantic TU/Year 4/Predictive Analysis Docu's/THE MODEL/French bakery daily sales/Iterations Output/french_bakery_Ft_Eng_1.csv\"\n",
    "\n",
    "iteration_file_path = \"C:/Users/regi4/OneDrive - Atlantic TU/Year 4/Predictive Analysis Docu's/THE MODEL/French bakery daily sales/Iterations Output\"\n",
    "results_file_path = \"C:/Users/regi4/OneDrive - Atlantic TU/Year 4/Predictive Analysis Docu's/THE MODEL/French bakery daily sales/results\"\n",
    "\n",
    "#   Import the dataset\n",
    "data = pd.read_csv(file_path_EDA)\n",
    "bakery_df = pd.DataFrame(data)\n",
    "bakery_df.head()\n",
    "\n",
    "#   Function to assign the season based on the date variable\n",
    "def get_season(date):\n",
    "    if (date.month == 12 and date.day >= 21) or (date.month <= 3 and date.day <= 19):\n",
    "        return \"Winter\"\n",
    "    elif (date.month == 3 and date.day >= 20) or (date.month <= 6 and date.day <= 20):\n",
    "        return \"Spring\"\n",
    "    elif (date.month == 6 and date.day >= 21) or (date.month <= 9 and date.day <= 22):\n",
    "        return \"Summer\"\n",
    "    else:\n",
    "        return \"Autumn\"\n",
    "    \n",
    "bakery_df =  bakery_df.drop(\"ticket_number\", axis='columns')\n",
    "bakery_df.head()\n",
    "\n",
    "#   Agregate the Quantity sold per article and day\n",
    "daily_data = data.groupby(['date', 'article']).agg({\n",
    "    'Quantity':'sum',\n",
    "    'unit_price': 'mean'\n",
    "}).reset_index()\n",
    "bakery_df = daily_data\n",
    "bakery_df.head()\n",
    "\n",
    "#   Convert the \"date\" variable to DateTime\n",
    "bakery_df['date'] = pd.to_datetime(bakery_df['date'])\n",
    "\n",
    "season_order = [\"Winter\", \"Spring\", \"Summer\", \"Autumn\"]\n",
    "\n",
    "day_of_week_mapping = {\n",
    "    \"Monday\": 0,\n",
    "    \"Tuesday\": 1,\n",
    "    \"Wednesday\": 2,\n",
    "    \"Thursday\": 3,\n",
    "    \"Friday\": 4,\n",
    "    \"Saturday\": 5,\n",
    "    \"Sunday\": 6\n",
    "}\n",
    "season_mapping = {\n",
    "    \"Winter\": 0,\n",
    "    \"Spring\": 1,\n",
    "    \"Summer\": 2,\n",
    "    \"Autumn\": 3\n",
    "}\n",
    "\n",
    "#   New Date features\n",
    "bakery_df['day_of_the_week'] = bakery_df['date'].dt.day_of_week\n",
    "bakery_df['month'] = bakery_df['date'].dt.month\n",
    "bakery_df['season'] = bakery_df['date'].apply(get_season)\n",
    "bakery_df['season_numeric'] = bakery_df['season'].map(season_mapping)\n",
    "\n",
    "#   Encoding Categorical Variables\n",
    "label_encoder_article = LabelEncoder()\n",
    "bakery_df['article_encoded'] = label_encoder_article.fit_transform(bakery_df['article'])\n",
    "\n",
    "#   Introducing Interaction Features to the dataset\n",
    "def create_interaction_features(bakery_df):\n",
    "    data_int_ft = bakery_df\n",
    "\n",
    "    data_int_ft['price_article_interaction'] = data_int_ft['unit_price'] * data_int_ft['article_encoded']\n",
    "    data_int_ft['season_article_interaction'] = data_int_ft['season_numeric'] * data_int_ft['article_encoded']\n",
    "    data_int_ft['weekly_avg_sales'] = data_int_ft.groupby(['article_encoded', 'day_of_the_week'])['Quantity'].transform('mean')  \n",
    "    data_int_ft['monthly_avg_sales'] = data_int_ft.groupby(['article_encoded', 'month'])['Quantity'].transform('mean') \n",
    "\n",
    "    return data_int_ft\n",
    "\n",
    "bakery_df = create_interaction_features(bakery_df)\n",
    "bakery_df.to_csv(f'{iteration_file_path}/french_bakery_Daily_Sales.csv', index=False)\n",
    "bakery_df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9247f9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import holidays\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, KFold, RandomizedSearchCV, ParameterGrid, TimeSeriesSplit, train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "127daa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   File paths\n",
    "file_path_EDA = \"C:/Users/regi4/OneDrive - Atlantic TU/Year 4/Predictive Analysis Docu's/THE MODEL/French bakery daily sales/Iterations Output/french_bakery_Daily_Sales.csv\"\n",
    "file_path_Feat_Engr = \"C:/Users/regi4/OneDrive - Atlantic TU/Year 4/Predictive Analysis Docu's/THE MODEL/French bakery daily sales/Iterations Output/french_bakery_Ft_Eng_1.csv\"\n",
    "\n",
    "iteration_file_path = \"C:/Users/regi4/OneDrive - Atlantic TU/Year 4/Predictive Analysis Docu's/THE MODEL/French bakery daily sales/Iterations Output\"\n",
    "results_file_path = \"C:/Users/regi4/OneDrive - Atlantic TU/Year 4/Predictive Analysis Docu's/THE MODEL/French bakery daily sales/results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c7efc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>article</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>unit_price</th>\n",
       "      <th>day_of_the_week</th>\n",
       "      <th>month</th>\n",
       "      <th>season</th>\n",
       "      <th>season_numeric</th>\n",
       "      <th>article_encoded</th>\n",
       "      <th>price_article_interaction</th>\n",
       "      <th>season_article_interaction</th>\n",
       "      <th>weekly_avg_sales</th>\n",
       "      <th>monthly_avg_sales</th>\n",
       "      <th>log_Quantity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-01-02</td>\n",
       "      <td>BAGUETTE</td>\n",
       "      <td>46</td>\n",
       "      <td>0.90</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Winter</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.7</td>\n",
       "      <td>0</td>\n",
       "      <td>41.988889</td>\n",
       "      <td>26.250000</td>\n",
       "      <td>3.850148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-01-02</td>\n",
       "      <td>BANETTE</td>\n",
       "      <td>40</td>\n",
       "      <td>1.05</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Winter</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6.3</td>\n",
       "      <td>0</td>\n",
       "      <td>43.088889</td>\n",
       "      <td>23.326923</td>\n",
       "      <td>3.713572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-01-02</td>\n",
       "      <td>BANETTINE</td>\n",
       "      <td>6</td>\n",
       "      <td>0.60</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Winter</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>4.2</td>\n",
       "      <td>0</td>\n",
       "      <td>5.179775</td>\n",
       "      <td>4.215686</td>\n",
       "      <td>1.945910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-01-02</td>\n",
       "      <td>BOULE 200G</td>\n",
       "      <td>6</td>\n",
       "      <td>1.10</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Winter</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.552941</td>\n",
       "      <td>3.340426</td>\n",
       "      <td>1.945910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-01-02</td>\n",
       "      <td>BOULE 400G</td>\n",
       "      <td>11</td>\n",
       "      <td>1.50</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Winter</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>16.5</td>\n",
       "      <td>0</td>\n",
       "      <td>9.277778</td>\n",
       "      <td>4.942308</td>\n",
       "      <td>2.484907</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date     article  Quantity  unit_price  day_of_the_week  month  \\\n",
       "0 2021-01-02    BAGUETTE        46        0.90                5      1   \n",
       "1 2021-01-02     BANETTE        40        1.05                5      1   \n",
       "2 2021-01-02   BANETTINE         6        0.60                5      1   \n",
       "3 2021-01-02  BOULE 200G         6        1.10                5      1   \n",
       "4 2021-01-02  BOULE 400G        11        1.50                5      1   \n",
       "\n",
       "   season  season_numeric  article_encoded  price_article_interaction  \\\n",
       "0  Winter               0                3                        2.7   \n",
       "1  Winter               0                6                        6.3   \n",
       "2  Winter               0                7                        4.2   \n",
       "3  Winter               0               10                       11.0   \n",
       "4  Winter               0               11                       16.5   \n",
       "\n",
       "   season_article_interaction  weekly_avg_sales  monthly_avg_sales  \\\n",
       "0                           0         41.988889          26.250000   \n",
       "1                           0         43.088889          23.326923   \n",
       "2                           0          5.179775           4.215686   \n",
       "3                           0          5.552941           3.340426   \n",
       "4                           0          9.277778           4.942308   \n",
       "\n",
       "   log_Quantity  \n",
       "0      3.850148  \n",
       "1      3.713572  \n",
       "2      1.945910  \n",
       "3      1.945910  \n",
       "4      2.484907  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(file_path_EDA)\n",
    "bakery_df = pd.DataFrame(data)\n",
    "\n",
    "bakery_df['date'] = pd.to_datetime(bakery_df['date'])\n",
    "bakery_df['Quantity'] = bakery_df['Quantity'].astype(int)\n",
    "bakery_df['log_Quantity'] = np.log1p(bakery_df['Quantity'])\n",
    "\n",
    "bakery_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b8481e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Introducing Interaction Features to the dataset\n",
    "def create_interaction_features(bakery_ft_df):\n",
    "    data_int_ft = bakery_ft_df\n",
    "    \n",
    "    data_int_ft['log_weekly_avg_sales'] = data_int_ft.groupby(['article_encoded', 'day_of_the_week'])['log_Quantity'].transform('mean')  \n",
    "    data_int_ft['log_monthly_avg_sales'] = data_int_ft.groupby(['article_encoded', 'month'])['log_Quantity'].transform('mean')\n",
    "\n",
    "    return data_int_ft\n",
    "\n",
    "# Function to calculate the SMAPE\n",
    "#Unlike MAPE, which cannot handle zeros, SMAPE is defined even when actual values are zero.\n",
    "def symmetric_mean_absolute_percentage_error(y_true, y_pred):\n",
    "    return 100 * (abs(y_true - y_pred) / ((abs(y_true) + abs(y_pred)) / 2)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d809a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   RandomizedSearchCV for hyperparameter Tuning\n",
    "def randomized_search_hyperparameters(model, param_grid, X_train, y_train, n_iter):\n",
    "    #Creating the time-based cross-validator\n",
    "    timebasedcv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "    randomized_search = RandomizedSearchCV(\n",
    "        estimator= model,\n",
    "        param_distributions= param_grid,\n",
    "        n_iter= n_iter,  #   Number of parameter settings to sample\n",
    "        scoring = 'neg_mean_absolute_error',\n",
    "        cv= timebasedcv,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    randomized_search.fit(X_train, y_train)\n",
    "\n",
    "    return randomized_search.best_estimator_, randomized_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27de578e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   CREATING LAG FEATURES (DAILY, WEEKLY, MONTHLY)\n",
    "def create_lag_features(bakery_ft_df):\n",
    "    \n",
    "    bakery_ft_df = bakery_ft_df.sort_values(by=['article_encoded', 'date'])\n",
    "    bakery_ft_df.reset_index(drop=True, inplace=True)\n",
    "    #   Create lag features\n",
    "    bakery_ft_df['lag_1'] = bakery_ft_df.groupby('article_encoded')['log_Quantity'].shift(1)\n",
    "    bakery_ft_df['lag_7'] = bakery_ft_df.groupby('article_encoded')['log_Quantity'].shift(7)\n",
    "    bakery_ft_df['lag_30'] = bakery_ft_df.groupby('article_encoded')['log_Quantity'].shift(30)    \n",
    "    \n",
    "    bakery_ft_df[['lag_1', 'lag_7', 'lag_30']] = bakery_ft_df.groupby('article_encoded')[['lag_1', 'lag_7', 'lag_30']].ffill().bfill()\n",
    "    return bakery_ft_df\n",
    "\n",
    "#   CREATING MOVING AVERAGES\n",
    "def create_moving_averages(bakery_ft_df):\n",
    "    bakery_ft_df = bakery_ft_df.sort_values(by=['article_encoded', 'date']).reset_index(drop=True)\n",
    "\n",
    "    bakery_ft_df['log_ma_7'] = bakery_ft_df.groupby('article_encoded')['log_Quantity'].transform(lambda x: x.rolling(window=7, min_periods=1).mean())\n",
    "    bakery_ft_df['log_ma_30'] = bakery_ft_df.groupby('article_encoded')['log_Quantity'].transform(lambda x: x.rolling(window=30, min_periods=1).mean())\n",
    "\n",
    "    return bakery_ft_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e390de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Evaluate XGBoost with transformations (NO REVERSE)\n",
    "\n",
    "def evaluate_xgboost(model, X_train_scaled, y_train, X_validate_scaled, y_validate, X_test_hold_scaled, y_test, cv):\n",
    "    xgb_cv_scores = []\n",
    "\n",
    "    for train_idx, val_idx in cv.split(X_train_scaled):\n",
    "        X_fold_train, X_fold_val = X_train_scaled.iloc[train_idx], X_train_scaled.iloc[val_idx]\n",
    "        y_fold_train, y_fold_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "        \n",
    "        #   Refit Scaler within each fold to avoid leakage\n",
    "        scaler_fold = StandardScaler()\n",
    "        X_fold_train = scaler_fold.fit_transform(X_fold_train)\n",
    "        X_fold_val = scaler_fold.transform(X_fold_val)\n",
    "\n",
    "        model.fit(X_fold_train, y_fold_train)\n",
    "        y_fold_pred = model.predict(X_fold_val)\n",
    "        fold_mae = mean_absolute_error(y_fold_val, y_fold_pred)\n",
    "        xgb_cv_scores.append(fold_mae)\n",
    "    \n",
    "    mean_cv_score = np.mean(xgb_cv_scores)\n",
    "\n",
    "    #   Fit the model on the full training data\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    #   Predict for the validation and test datasets\n",
    "    y_validate_pred = model.predict(X_validate_scaled)\n",
    "    y_test_pred = model.predict(X_test_hold_scaled)\n",
    "\n",
    "    metrics = {\n",
    "        \"CV Mean MAE\": mean_cv_score,\n",
    "        \"Val MAE\": mean_absolute_error(y_validate, y_validate_pred),\n",
    "        \"Val MSE\": mean_squared_error(y_validate, y_validate_pred),\n",
    "        \"Val R2\": r2_score(y_validate, y_validate_pred),\n",
    "        \"Val SMAPE\": symmetric_mean_absolute_percentage_error(y_validate, y_validate_pred),\n",
    "        \"Test MAE\": mean_absolute_error(y_test, y_test_pred),\n",
    "        \"Test MSE\": mean_squared_error(y_test, y_test_pred),\n",
    "        \"Test R2\": r2_score(y_test, y_test_pred),\n",
    "        \"Test SMAPE\": symmetric_mean_absolute_percentage_error(y_test, y_test_pred)\n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d55953",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   TO REVERT THE LOG RESULTS TO ORIGINAL SCALE AND THEN TEST THE PERFORMANCE\n",
    "def evaluate_ORIG_xgboost(model, X_train_scaled, y_train, X_validate_scaled, y_validate, X_test_hold_scaled, y_test, cv):\n",
    "    xgb_cv_scores = []\n",
    "\n",
    "    for train_idx, val_idx in cv.split(X_train_scaled):\n",
    "        X_fold_train, X_fold_val = X_train_scaled.iloc[train_idx], X_train_scaled.iloc[val_idx]\n",
    "        y_fold_train, y_fold_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "        \n",
    "        scaler_fold = StandardScaler()\n",
    "        X_fold_train = scaler_fold.fit_transform(X_fold_train)\n",
    "        X_fold_val = scaler_fold.transform(X_fold_val)\n",
    "\n",
    "        model.fit(X_fold_train, y_fold_train)\n",
    "        y_fold_pred = model.predict(X_fold_val)\n",
    "        \n",
    "        # Inverse transform predictions and true values from log scale to original scale\n",
    "        y_fold_pred_orig = np.expm1(y_fold_pred)\n",
    "        y_fold_val_orig = np.expm1(y_fold_val)\n",
    "        \n",
    "        fold_mae = mean_absolute_error(y_fold_val_orig, y_fold_pred_orig)\n",
    "        xgb_cv_scores.append(fold_mae)\n",
    "    \n",
    "    mean_cv_score = np.mean(xgb_cv_scores)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    y_validate_pred = model.predict(X_validate_scaled)\n",
    "    y_test_pred = model.predict(X_test_hold_scaled)\n",
    "    \n",
    "    # Convert predictions and targets back to the original scale\n",
    "    y_validate_pred_orig = np.expm1(y_validate_pred)\n",
    "    y_validate_orig = np.expm1(y_validate)\n",
    "    y_test_pred_orig = np.expm1(y_test_pred)\n",
    "    y_test_orig = np.expm1(y_test)\n",
    "    \n",
    "    metrics = {\n",
    "        \"CV Mean MAE (orig)\": mean_cv_score,\n",
    "        \"Val MAE (orig)\": mean_absolute_error(y_validate_orig, y_validate_pred_orig),\n",
    "        \"Val MSE (orig)\": mean_squared_error(y_validate_orig, y_validate_pred_orig),\n",
    "        \"Val R2 (orig)\": r2_score(y_validate_orig, y_validate_pred_orig),\n",
    "        \"Val SMAPE (orig)\": symmetric_mean_absolute_percentage_error(y_validate_orig, y_validate_pred_orig),\n",
    "        \"Test MAE (orig)\": mean_absolute_error(y_test_orig, y_test_pred_orig),\n",
    "        \"Test MSE (orig)\": mean_squared_error(y_test_orig, y_test_pred_orig),\n",
    "        \"Test R2 (orig)\": r2_score(y_test_orig, y_test_pred_orig),\n",
    "        \"Test SMAPE (orig)\": symmetric_mean_absolute_percentage_error(y_test_orig, y_test_pred_orig)\n",
    "    }\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7cbbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(models, features, X_train_scaled, y_train, X_validate_scaled, y_validate, X_test_hold_scaled, y_test, cv):\n",
    "    results = []\n",
    "    for model_name, model in models.items():\n",
    "        metrics = evaluate_ORIG_xgboost(model, X_train_scaled, y_train, X_validate_scaled, y_validate, X_test_hold_scaled, y_test, cv)\n",
    "        \n",
    "        metrics[\"Model\"] = model_name\n",
    "        results.append(metrics)\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "195fe3ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 28597 entries, 0 to 28596\n",
      "Data columns (total 21 columns):\n",
      " #   Column                      Non-Null Count  Dtype         \n",
      "---  ------                      --------------  -----         \n",
      " 0   date                        28597 non-null  datetime64[ns]\n",
      " 1   article                     28597 non-null  object        \n",
      " 2   Quantity                    28597 non-null  int64         \n",
      " 3   unit_price                  28597 non-null  float64       \n",
      " 4   day_of_the_week             28597 non-null  int64         \n",
      " 5   month                       28597 non-null  int64         \n",
      " 6   season                      28597 non-null  object        \n",
      " 7   season_numeric              28597 non-null  int64         \n",
      " 8   article_encoded             28597 non-null  int64         \n",
      " 9   price_article_interaction   28597 non-null  float64       \n",
      " 10  season_article_interaction  28597 non-null  int64         \n",
      " 11  weekly_avg_sales            28597 non-null  float64       \n",
      " 12  monthly_avg_sales           28597 non-null  float64       \n",
      " 13  log_Quantity                28597 non-null  float64       \n",
      " 14  log_weekly_avg_sales        28597 non-null  float64       \n",
      " 15  log_monthly_avg_sales       28597 non-null  float64       \n",
      " 16  lag_1                       28597 non-null  float64       \n",
      " 17  lag_7                       28597 non-null  float64       \n",
      " 18  lag_30                      28597 non-null  float64       \n",
      " 19  log_ma_7                    28597 non-null  float64       \n",
      " 20  log_ma_30                   28597 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(12), int64(6), object(2)\n",
      "memory usage: 4.6+ MB\n"
     ]
    }
   ],
   "source": [
    "xgb_parameter_grid = {\n",
    "'n_estimators': [50, 100, 200, 300],\n",
    "    'max_depth': [3, 6, 10, 15],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'subsample': [0.6, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "bakery_df = create_interaction_features(bakery_df)\n",
    "bakery_df = create_lag_features(bakery_df)\n",
    "bakery_df = create_moving_averages(bakery_df)\n",
    "\n",
    "bakery_df.sort_values(by=\"date\")\n",
    "bakery_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#   Feature and Target selection with ENCODED FEATURES\n",
    "features = bakery_df[['day_of_the_week', 'article_encoded', 'unit_price', 'month', 'season_numeric', 'price_article_interaction', 'season_article_interaction', 'log_weekly_avg_sales',\n",
    "                       'log_monthly_avg_sales', 'lag_1','lag_7','lag_30', 'log_ma_7', 'log_ma_30']]\n",
    "target = bakery_df['log_Quantity']\n",
    "\n",
    "bakery_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e9cd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   TIME SERIES SPLIT FOR EXACTLY ONE YEAR\n",
    "\n",
    "bakery_df[\"date\"] = pd.to_datetime(bakery_df[\"date\"])\n",
    "start_date = bakery_df[\"date\"].min()\n",
    "cutoff_date = start_date+pd.DateOffset(years=1)\n",
    "\n",
    "training_data = bakery_df[bakery_df[\"date\"] < cutoff_date]\n",
    "testing_data = bakery_df[bakery_df[\"date\"] >= cutoff_date]\n",
    "\n",
    "X_train_all = training_data[features.columns]\n",
    "y_train_all = training_data[target.name]\n",
    "X_test_hold = testing_data[features.columns]\n",
    "y_test_hold = testing_data[target.name]\n",
    "\n",
    "#   Splitting the training data into train and validation sets\n",
    "val_split_index = int(len(X_train_all) * 0.8)\n",
    "X_train = X_train_all.iloc[:val_split_index]\n",
    "y_train = y_train_all.iloc[:val_split_index]\n",
    "X_validate = X_train_all[val_split_index:]\n",
    "y_validate = y_train_all.iloc[val_split_index:]\n",
    "\n",
    "#   Fitting scaler on train set then transforming the val and test with same scale\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_validate_scaled = scaler.transform(X_validate)\n",
    "X_test_hold_scaled = scaler.transform(X_test_hold)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1311ae64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Convert the training sets to Dataframe, because X_train_scaled is a numpy array           # SCALING CODE\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=features.columns)                              # SCALING CODE\n",
    "X_validate_scaled = pd.DataFrame(X_validate_scaled, columns=features.columns)                        # SCALING CODE\n",
    "X_test_hold_scaled = pd.DataFrame(X_test_hold_scaled, columns=features.columns)                      # SCALING CODE\n",
    "\n",
    "xgb_n_iter = min(20, len(ParameterGrid(xgb_parameter_grid)))\n",
    "best_xgb, xgb_best_params = randomized_search_hyperparameters(XGBRegressor(), xgb_parameter_grid, X_train, y_train, xgb_n_iter)\n",
    "\n",
    "# Define model best hyperparameters\n",
    "models = {\n",
    "    \"XGBoost Regressor\": best_xgb\n",
    "}\n",
    "\n",
    "timeBased_final_cv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Evaluate model\n",
    "results_df = evaluate_model(models, features, X_train_scaled, y_train, X_validate_scaled, y_validate, X_test_hold_scaled, y_test_hold, timeBased_final_cv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7458f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "results_df.to_csv(f\"{results_file_path}/2_Daily_ORIG_time_series_split_ONE_YEAR_LOG_QUANTITY_LAG_ALL_MA_features_result.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fcbcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_articles = bakery_df['article_encoded'].unique()\n",
    "\n",
    "from datetime import timedelta\n",
    "\n",
    "# Get the next day after the end of the dataset\n",
    "next_day = bakery_df[\"date\"].max() + timedelta(days=1)\n",
    "day_of_week = next_day.dayofweek\n",
    "month = next_day.month\n",
    "season = (month%12 + 3)//3\n",
    "\n",
    "feature_rows = []\n",
    "\n",
    "for article in unique_articles:\n",
    "    # Get recent data for the article\n",
    "    article_data = bakery_df[bakery_df['article_encoded'] == article].sort_values('date')\n",
    "    \n",
    "    if len(article_data) < 30:\n",
    "        continue \n",
    "\n",
    "    last_row = article_data.iloc[-1]\n",
    "    \n",
    "    # Generate the lags and moving averages for the future date manually\n",
    "    recent_sales = article_data['log_Quantity'].values\n",
    "    lag_1 = recent_sales[-1]\n",
    "    lag_7 = recent_sales[-7]\n",
    "    lag_30 = recent_sales[-30]\n",
    "    \n",
    "    ma_7 = np.mean(recent_sales[-7:])\n",
    "    ma_30 = np.mean(recent_sales[-30:])\n",
    "    \n",
    "    # Creting the row for next-day prediction\n",
    "    row = {\n",
    "        'day_of_the_week': day_of_week,\n",
    "        'article_encoded': article,\n",
    "        'unit_price': last_row['unit_price'],\n",
    "        'month': month,\n",
    "        'season_numeric': season,\n",
    "        'price_article_interaction': last_row['unit_price'] * article,\n",
    "        'season_article_interaction': season * article,\n",
    "        'log_weekly_avg_sales': last_row['log_weekly_avg_sales'],\n",
    "        'log_monthly_avg_sales': last_row['log_monthly_avg_sales'],\n",
    "        'lag_1': lag_1,\n",
    "        'lag_7': lag_7,\n",
    "        'lag_30': lag_30,\n",
    "        'log_ma_7': ma_7,\n",
    "        'log_ma_30': ma_30\n",
    "    }\n",
    "    \n",
    "    feature_rows.append(row)\n",
    "\n",
    "next_day_df = pd.DataFrame(feature_rows)\n",
    "\n",
    "# Scaling using the existing scaler\n",
    "next_day_scaled = scaler.transform(next_day_df[features.columns])\n",
    "\n",
    "next_day_log_preds = best_xgb.predict(next_day_scaled)\n",
    "next_day_preds = np.expm1(next_day_log_preds)\n",
    "\n",
    "next_day_df['Predicted_Quantity'] = next_day_preds\n",
    "next_day_df['date'] = next_day\n",
    "\n",
    "# Undoing the label encoding on the articles to get their name\n",
    "next_day_df['article'] = label_encoder_article.inverse_transform(next_day_df['article_encoded'])\n",
    "\n",
    "next_day_df[['date', 'article', 'Predicted_Quantity']].to_csv(f\"{results_file_path}/Tomorrow_Predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e76723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: C:/Users/regi4/OneDrive - Atlantic TU/Year 4/Predictive Analysis Docu's/THE MODEL/French bakery daily sales/results/Tomorrow_Predictions.csv → s3://test-pred-bucket/Predictions.csv\n"
     ]
    }
   ],
   "source": [
    "#   SEND PREDICTIONS/FILE TO S3 BUCKET\n",
    "import logging\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "def upload_file(file_path, bucket, object_name):\n",
    "    s3_client = boto3.client('s3',\n",
    "                             aws_access_key_id=\"aws-access-key-id\",\n",
    "                             aws_secret_access_key=\"aws-secret-access-key\",\n",
    "                             region_name=\"eu-west-1\")\n",
    "    try:\n",
    "        response = s3_client.upload_file(file_path, bucket, object_name)\n",
    "        print(f\"Upload successful: {file_path} → s3://{bucket}/{object_name}\")\n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        logging.error(e)\n",
    "        print(f\"Upload failed: {e}\")\n",
    "        return False\n",
    "\n",
    "output_file = \"Tomorrow_Predictions.csv\"\n",
    "s3bucket = \"test-pred-bucket\"\n",
    "file_path = f\"{results_file_path}/{output_file}\"\n",
    "object_name = \"Predictions.csv\"\n",
    "upload_success = upload_file(file_path, s3bucket, object_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
